{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Active Learning Demo\n",
    "\n",
    "This notebook demonstrates the REINFORCE-based active learning agent for Dogs vs Cats classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from src.models.cnn import ResNet18Binary\n",
    "from src.rl.policy import PolicyNetwork\n",
    "from src.datasets.loaders import get_dataloaders\n",
    "from src.utils.confidence import get_confidence_metrics\n",
    "from src.utils.evaluation import track_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained classifier and policy\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load classifier\n",
    "classifier = ResNet18Binary(pretrained=True).to(device)\n",
    "classifier.load_state_dict(torch.load('../checkpoints/week2/resnet18_epoch5.pth', map_location=device))\n",
    "\n",
    "# Load policy\n",
    "policy = PolicyNetwork(in_dim=512, hidden=256).to(device)\n",
    "policy.load_state_dict(torch.load('../checkpoints/week4/policy_epoch5.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Selection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "_, val_loader, _, val_ds = get_dataloaders('../data/processed/catsdogs_128', batch_size=32)\n",
    "\n",
    "# Get confidence metrics\n",
    "metrics = get_confidence_metrics(classifier, val_loader, device)\n",
    "\n",
    "# Visualize entropy distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(metrics['entropy'].numpy(), bins=50, alpha=0.7)\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Uncertainty Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(metrics['margin'].numpy(), bins=50, alpha=0.7)\n",
    "plt.xlabel('Margin')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Confidence Margin Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Policy scores for features\n",
    "with torch.no_grad():\n",
    "    policy_scores = policy(metrics['features'][:1000].to(device)).cpu()\n",
    "plt.hist(policy_scores.numpy(), bins=50, alpha=0.7)\n",
    "plt.xlabel('Policy Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Policy Selection Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from all weeks\n",
    "import json\n",
    "\n",
    "# Load Week 3 results (baselines)\n",
    "with open('../outputs/week3/curves.json', 'r') as f:\n",
    "    week3_curves = json.load(f)\n",
    "\n",
    "# Load Week 4 results (RL)\n",
    "with open('../outputs/week4/rl_curve.json', 'r') as f:\n",
    "    week4_curves = json.load(f)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot baselines\n",
    "for method, accuracies in week3_curves.items():\n",
    "    rounds = list(range(len(accuracies)))\n",
    "    labeled_counts = [1000 + r * 500 for r in rounds]\n",
    "    plt.plot(labeled_counts, accuracies, label=f'{method} Sampling', marker='o')\n",
    "\n",
    "# Plot RL curve\n",
    "rl_accuracies = week4_curves['RL']\n",
    "rl_steps = list(range(len(rl_accuracies)))\n",
    "rl_labeled_counts = [1000 + (s // 9) * 500 for s in rl_steps]  # Approximate mapping\n",
    "plt.plot(rl_labeled_counts[:len(rl_accuracies)], rl_accuracies, label='REINFORCE Policy', marker='s', linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Active Learning Comparison: Policy Gradient vs Traditional Methods')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print sample efficiency\n",
    "print(\"\\nSample Efficiency Analysis:\")\n",
    "print(\"Method\\t\\t\\tSamples to 90% Accuracy\")\n",
    "print(\"-\" * 40)\n",
    "for method, accuracies in week3_curves.items():\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        if acc >= 0.90:\n",
    "            samples = 1000 + i * 500\n",
    "            print(f\"{method}\\t\\t\\t{samples}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"{method}\\t\\t\\tNot reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what the policy has learned\n",
    "with torch.no_grad():\n",
    "    # Get policy scores for different confidence levels\n",
    "    high_entropy_idx = torch.argsort(metrics['entropy'], descending=True)[:100]\n",
    "    low_entropy_idx = torch.argsort(metrics['entropy'], descending=False)[:100]\n",
    "    \n",
    "    high_entropy_features = metrics['features'][high_entropy_idx].to(device)\n",
    "    low_entropy_features = metrics['features'][low_entropy_idx].to(device)\n",
    "    \n",
    "    high_entropy_scores = policy(high_entropy_features).cpu()\n",
    "    low_entropy_scores = policy(low_entropy_features).cpu()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(metrics['entropy'][high_entropy_idx], high_entropy_scores, alpha=0.6, label='High Entropy')\n",
    "plt.scatter(metrics['entropy'][low_entropy_idx], low_entropy_scores, alpha=0.6, label='Low Entropy')\n",
    "plt.xlabel('Sample Entropy')\n",
    "plt.ylabel('Policy Score')\n",
    "plt.title('Policy Score vs Sample Uncertainty')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([high_entropy_scores.numpy(), low_entropy_scores.numpy()], \n",
    "           labels=['High Entropy\\n(Uncertain)', 'Low Entropy\\n(Confident)'])\n",
    "plt.ylabel('Policy Score')\n",
    "plt.title('Policy Preference Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean policy score for high entropy samples: {high_entropy_scores.mean():.3f}\")\n",
    "print(f\"Mean policy score for low entropy samples: {low_entropy_scores.mean():.3f}\")\n",
    "print(f\"Policy preference ratio (high/low): {high_entropy_scores.mean() / low_entropy_scores.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}